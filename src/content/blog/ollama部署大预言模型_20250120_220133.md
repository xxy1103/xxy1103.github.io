---
title: ollama部署大预言模型
date: 2025-01-20 22:01:33
categories: 记录
tags: ollama
---
# 每日一言

It's like I'd want to ask what justice is doing right now if it existed in this world. Well, justice is empty and ineffectual. -- Senjougahara Hitagi
from Monogatari Series: Second Season

<!-- more -->

# ollama

ollama几乎是市面上最简单的本地部署本地大模型的开源项目，可以轻松的下载，切换和管理不同的LLM模型，提供简单的命令行界面，支持macOS，Linux，Windows，系统，并且可以通过API集成到其他应用中。

# 部署ollama

ollama的部署方式非常的简单，直接到ollama的官网，选择合适的系统版本，直接按照提示下载安装即可：[Ollama](https://ollama.com/)

![1737382104716](image/ollama部署大预言模型_20250120_220133/1737382104716.webp)

下载安装后，在命令行中输入命令：`ollama -v`

![1737382234814](image/ollama部署大预言模型_20250120_220133/1737382234814.webp)

显示ollama的版本即为安装成功。

然后在ollama的官网中选择合适的大模型：

![1737382497600](image/ollama部署大预言模型_20250120_220133/1737382497600.webp)

选择好合适的大模型后：

![1737382786679](image/ollama部署大预言模型_20250120_220133/1737382786679.webp)

复制框出的代码，在终端中复制粘贴：

![1737382853297](image/ollama部署大预言模型_20250120_220133/1737382853297.webp)

就会开始自定下载该模型，下载完毕后会自动启动该模型。

# 配置ollama

## 模型下载位置

ollama一般自动将模型下载到c盘路径：`C:\Users\%username%\.ollama\models`

`%username%` 是一个 **环境变量** ，它代表当前登录Windows系统的用户的 **用户名** 。你无需手动替换为你的用户名，系统会自动将他替换为你当前登录用户的用户名。

我们可以通过手动添加环境变量：`OLLAMA_MODELS`来修改ollama模型下载的地址。

![1737383585516](image/ollama部署大预言模型_20250120_220133/1737383585516.webp)

Value中填写你希望的文件夹路径，之后你就可以在对应路径文件夹下找到ollama下载的模型。

## API提供服务

一般Ollama仅在本地运行，但是大模型常用的一种调用方法是网络API调用，对外提供服务可以让你将模型部署在服务器中，同时共局域网中的其他设备使用。

提供对外服务依然需要我们设置两个环境变量：

### 监听计算机所有网络ip

我们设置环境变量：`OLLAMA_HOST = 0.0.0.0`

![1737383911893](image/ollama部署大预言模型_20250120_220133/1737383911893.webp)

`0.0.0.0` 是一个特殊的 IP 地址，在网络编程中具有特殊的含义。当一个服务监听 `0.0.0.0` 时，它实际上意味着它会监听计算机上 **所有可用的网络接口** 。这包括:

* **本地回环地址 (127.0.0.1)：** 用于本地计算机内部的通信。
* **计算机的本地 IP 地址 (例如 192.168.x.x)：** 用于局域网内的通信。
* **可能的公共 IP 地址：** 如果计算机连接到互联网。

当你设定为特定的ip地址时，ollama将只监听你输入的ip，**监听的是自己电脑的ip**。

### 允许所有的访问请求

设置环境变量：`OLLAMA_ORIGINS = *`

![1737384368477](image/ollama部署大预言模型_20250120_220133/1737384368477.webp)

origins: 起点，来源

该参数设定的是，只接受指定来自指定地址的请求，**要求的是对方的地址**，如：

* `OLLAMA_ORIGINS=http://localhost:3000`：只允许来自 `http://localhost:3000` 的请求。

设定为 `*`即为接收来自所有地址的访问。

### 总结

结合 `OLLAMA_HOST=0.0.0.0` 和 `OLLAMA_ORIGINS=*`，你实际上是在告诉 Ollama 服务监听所有网络接口，并且接受来自任何网站的跨域请求。这可能会方便你开发和测试，但务必注意安全风险，并在生产环境中进行更严格的配置。

## 接入一个好用的前端页面

![1737385483317](image/ollama部署大预言模型_20250120_220133/1737385483317.webp)

直接在控制台中使用显然不太方便，我们既然已经打开api调用，就可以将他接入一些前端项目中：

推荐一：[🏡 Home | Open WebUI](https://docs.openwebui.com/)

推荐二：[Chatbox AI官网：办公学习的AI好助手，全平台AI客户端，官方免费下载](https://chatboxai.app/zh)

相对而言个人感觉第二个chatbox使用起来更简单一些，而且支持的功能很多。

![1737386369728](image/ollama部署大预言模型_20250120_220133/1737386369728.webp)

选择ollama，选择模型，就可以将ollma模型接入chatbox了。

![1737386498985](image/ollama部署大预言模型_20250120_220133/1737386498985.webp)
